# daily-readings
Tracking my progress of reading at least 1 AI-related research paper daily.
# ðŸ“š Daily Reading Challenge  

| ðŸ“… **Date** | ðŸ“– **Title** | ðŸ”— **Link** |  
|------------|-------------|------------|  
| 2024-08-17 | *LoRA: Low-Rank Adapation of Large Language Models* | [Read Here](https://arxiv.org/pdf/2106.09685) |
| 2024-09-18 | *LoRA-GA: Low-Rank Adaptation with Gradient Approximation* | [Read Here](https://arxiv.org/pdf/2407.05000) |
| 2024-09-21 | *Self-Consistency Improves Chain of Thought Reasoning in Language Models* | [Read Here](https://arxiv.org/pdf/2203.11171) |
| 2024-09-25 | *Have You Merged My Model? On The Robustness of Large Language Model IP Protection Methods Against Model Merging* | [Read Here](https://arxiv.org/pdf/2404.05188) |
| 2024-10-14 | *LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models* | [Read Here](https://arxiv.org/pdf/2308.16137) |
| 2024-10-14 | *Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality* | [Read Here](https://arxiv.org/pdf/2405.21060) |
| 2024-11-31 | *FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning* | [Read Here](https://arxiv.org/pdf/2307.08691) |
| 2025-01-31 | *Denoising Diffusion Probabilistic Models* | [Read Here](https://arxiv.org/pdf/2006.11239) |  
| 2025-01-31 | *Denoising Diffusion Implicit Models* | [Read Here](https://arxiv.org/pdf/2010.02502) |  
| 2025-02-01 | *Toy Models of Superposition* | [Read Here](https://transformer-circuits.pub/2022/toy_model/index.html) |  
| 2025-02-01 | *Towards Monosemanticity: Decomposing Language Models With Dictionary Learning* | [Read Here](https://transformer-circuits.pub/2023/monosemantic-features/index.html) |
| 2025-02-12 | *A Mathematical Framework for Transformer Circuits* | [Read Here](https://transformer-circuits.pub/2021/framework/index.html) |
| 2025-02-12 | *ICLR: In-Context Learning of Representations* | [Read Here](https://arxiv.org/pdf/2501.00070) |

